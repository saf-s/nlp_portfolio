{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Exploring NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK and its book collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting first 20 tokens from text1\n",
    "The code below extracts and displays the first 20 tokens from text1, an NLTK Text object, using the tokens() method.\n",
    "\n",
    "The tokens() method tokenizes strings by extracting words and ignoring whitespace. Text objects can be used to list each time a word occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " 'Moby',\n",
       " 'Dick',\n",
       " 'by',\n",
       " 'Herman',\n",
       " 'Melville',\n",
       " '1851',\n",
       " ']',\n",
       " 'ETYMOLOGY',\n",
       " '.',\n",
       " '(',\n",
       " 'Supplied',\n",
       " 'by',\n",
       " 'a',\n",
       " 'Late',\n",
       " 'Consumptive',\n",
       " 'Usher',\n",
       " 'to',\n",
       " 'a',\n",
       " 'Grammar']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Displaying 5 word occurrences from text1\n",
    "The code below lists 5 occurrences of the word \"sea\" with its surrounding context from text1, an NLTK Text object, using the concordance() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 455 matches:\n",
      " shall slay the dragon that is in the sea .\" -- ISAIAH \" And what thing soever \n",
      " S PLUTARCH ' S MORALS . \" The Indian Sea breedeth the most and the biggest fis\n",
      "cely had we proceeded two days on the sea , when about sunrise a great many Wha\n",
      "many Whales and other monsters of the sea , appeared . Among the former , one w\n",
      " waves on all sides , and beating the sea before him into a foam .\" -- TOOKE ' \n"
     ]
    }
   ],
   "source": [
    "text1.concordance(\"sea\", lines=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python vs API: count() method\n",
    "The code below shows that the Python count() method for a string of text returns the count of the number of times a given object occurs in the given text, while the count() method in the API (https://www.nltk.org/_modules/nltk/text.html) uses tokens to return the count of the number of times a specific word appears in a given text. Unlike the count() method in the API, the Python count() method can be used to return a count of the number of times a particular value occurs between positions in the string of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Python count() method:\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Python count() method:\")\n",
    "txt = \"Count this, this, this, and this. Count this, too.\"\n",
    "x = txt.count(\"this\", 10, 30)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API count() method:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Testing API count() method:\")\n",
    "text1.count(\"sea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing raw text into words\n",
    "The code below uses raw text of 6 sentences from https://fitnessgram.net/pacertest/, saves the text into the variable 'raw_text', uses NLTK's word tokenizer to tokenize the text into the variable 'tokens', and prints out the first 10 tokens of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FitnessGram', 'PACER', 'Test', 'is', 'a', 'multistage', 'aerobic', 'capacity', 'test']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "raw_text = \"The FitnessGram PACER Test is a multistage aerobic capacity test that progressively gets more difficult as it continues. The test is used to measure a student's aerobic capacity as part of the FitnessGram assessment. Students run back and forth as many times as they can, each lap signaled by a beep sound. The test get progressively faster as it continues until the student reaches their max lap score. The iconic voice of the original test has been replaced by both male and female voices in English and Spanish to motivate and encourage participants throughout each of the 22-minute tracks. The combination of diverse voices and culturally relevant beats is designed to inspire a new generation of students to get active and stay healthy now and well into adulthood.\"\n",
    "tokens = word_tokenize(raw_text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing raw text into sentences\n",
    "The code below uses the raw text from above and NLTK's sentence tokenizer sent_tokenize() to perform sentence segmentation and prints out the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The FitnessGram PACER Test is a multistage aerobic capacity test that progressively gets more difficult as it continues.', \"The test is used to measure a student's aerobic capacity as part of the FitnessGram assessment.\", 'Students run back and forth as many times as they can, each lap signaled by a beep sound.', 'The test get progressively faster as it continues until the student reaches their max lap score.', 'The iconic voice of the original test has been replaced by both male and female voices in English and Spanish to motivate and encourage participants throughout each of the 22-minute tracks.', 'The combination of diverse voices and culturally relevant beats is designed to inspire a new generation of students to get active and stay healthy now and well into adulthood.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(raw_text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming raw text\n",
    "The code below uses NLTK's PorterStemmer() to create a list comprehension to stem the raw text and displays the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fitnessgram', 'pacer', 'test', 'is', 'a', 'multistag', 'aerob', 'capac', 'test', 'that', 'progress', 'get', 'more', 'difficult', 'as', 'it', 'continu', '.', 'the', 'test', 'is', 'use', 'to', 'measur', 'a', 'student', \"'s\", 'aerob', 'capac', 'as', 'part', 'of', 'the', 'fitnessgram', 'assess', '.', 'student', 'run', 'back', 'and', 'forth', 'as', 'mani', 'time', 'as', 'they', 'can', ',', 'each', 'lap', 'signal', 'by', 'a', 'beep', 'sound', '.', 'the', 'test', 'get', 'progress', 'faster', 'as', 'it', 'continu', 'until', 'the', 'student', 'reach', 'their', 'max', 'lap', 'score', '.', 'the', 'icon', 'voic', 'of', 'the', 'origin', 'test', 'ha', 'been', 'replac', 'by', 'both', 'male', 'and', 'femal', 'voic', 'in', 'english', 'and', 'spanish', 'to', 'motiv', 'and', 'encourag', 'particip', 'throughout', 'each', 'of', 'the', '22-minut', 'track', '.', 'the', 'combin', 'of', 'divers', 'voic', 'and', 'cultur', 'relev', 'beat', 'is', 'design', 'to', 'inspir', 'a', 'new', 'gener', 'of', 'student', 'to', 'get', 'activ', 'and', 'stay', 'healthi', 'now', 'and', 'well', 'into', 'adulthood', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(t) for t in tokens]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing raw text\n",
    "The code below uses NLTK's WordNetLemmatizer to create a list comprehension to lemmatize the raw text and displays the resulting list.\n",
    "\n",
    "The output from stemming and lemmatizing the raw text shows that stems remove affixes like prefixes/suffixes from words, while lemmas convert text to the lexical form by searching for root words. While stems converted all text to lower case, lemmas maintained the capitalization of the raw text. Stems can result in tokens that do not form exact words from the text, so they may have different meanings when compared to the raw text. Stems help process text using the NLTK library's PorterStemmer() method, and lemmas use the library's WordNetLemmatizer() method. Stems and lemmas involve text normalization as they change related words into a standardized form for counting their number of occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'FitnessGram', 'PACER', 'Test', 'is', 'a', 'multistage', 'aerobic', 'capacity', 'test', 'that', 'progressively', 'get', 'more', 'difficult', 'a', 'it', 'continues', '.', 'The', 'test', 'is', 'used', 'to', 'measure', 'a', 'student', \"'s\", 'aerobic', 'capacity', 'a', 'part', 'of', 'the', 'FitnessGram', 'assessment', '.', 'Students', 'run', 'back', 'and', 'forth', 'a', 'many', 'time', 'a', 'they', 'can', ',', 'each', 'lap', 'signaled', 'by', 'a', 'beep', 'sound', '.', 'The', 'test', 'get', 'progressively', 'faster', 'a', 'it', 'continues', 'until', 'the', 'student', 'reach', 'their', 'max', 'lap', 'score', '.', 'The', 'iconic', 'voice', 'of', 'the', 'original', 'test', 'ha', 'been', 'replaced', 'by', 'both', 'male', 'and', 'female', 'voice', 'in', 'English', 'and', 'Spanish', 'to', 'motivate', 'and', 'encourage', 'participant', 'throughout', 'each', 'of', 'the', '22-minute', 'track', '.', 'The', 'combination', 'of', 'diverse', 'voice', 'and', 'culturally', 'relevant', 'beat', 'is', 'designed', 'to', 'inspire', 'a', 'new', 'generation', 'of', 'student', 'to', 'get', 'active', 'and', 'stay', 'healthy', 'now', 'and', 'well', 'into', 'adulthood', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized = [wnl.lemmatize(t) for t in tokens]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "The functionality and code quality of the NLTK library is useful, especially for text processing, as NLTK is an open-source library for Python and can be used for NLP functions such as tokenizing and text normalization such as stemming and lemmatizing. By experimenting with the different functions of the NLTK library, I can use NLTK in future projects such as translating written text to voice text, filtering spam emails, or spellcheckers for text processing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2ba252fe723dbf5b4005d1597e15f6f84190629abaf2b8c0ec9e30f106cffba4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
